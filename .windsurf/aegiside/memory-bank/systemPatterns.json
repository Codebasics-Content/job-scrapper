{
  "schema_version": "1.0.0",
  "last_updated": "2025-10-17T02:08:07+05:30",
  "metrics": {
    "rl_source_ref": "progress.json",
    "tasks_completed": 9,
    "tasks_failed": 1,
    "commits": 9,
    "rl_reward": -30,
    "success_rate": 0.9,
    "gae_advantage": 0.4,
    "value_branch": "task_success",
    "timestamp": "2025-10-17T02:08:07+05:30"
  },
  "architecture_patterns": [
    {
      "pattern_id": "basepyright-strict-typing-compliance",
      "category": "type_safety",
      "name": "Strict Type Hints with basepyright",
      "description": "Replace lambdas with explicit typed functions, import proper types from libraries, guard None values",
      "use_cases": [
        "Lambda sorting operations → explicit typed function",
        "Third-party library types → proper imports (ProxySettings, etc.)",
        "Environment variables → None guard before usage"
      ],
      "implementation": {
        "lambda_to_function": "def sort_key(param: tuple[str, float]) -> tuple[float, str]: return (-param[1], param[0])",
        "proper_imports": "from playwright.async_api import ProxySettings",
        "none_guards": "if not value: print('Error'); exit(1)"
      },
      "benefits": [
        "Zero basepyright type errors",
        "Better IDE autocomplete",
        "Safer runtime behavior",
        "Constitutional Article 15 compliance"
      ],
      "success_rate": 1.0,
      "confidence_score": 0.95,
      "reuse_count": 1,
      "rl_reward": 40,
      "last_used": "2025-10-16T13:20:42+05:30"
    },
    {
      "pattern_id": "rl_score_single_source_2025-10-15",
      "category": "reinforcement_learning",
      "name": "Single Source of Truth for RL Scores",
      "problem": "8 different total_rl_score values across schemas (-95, -40, -170, -250) caused sync issues and confusion",
      "solution": "progress.json = SINGLE SOURCE OF TRUTH. All other schemas use rl_source_ref field pointing to progress.json",
      "implementation": {
        "central_ledger": "progress.json stores total_rl_score in metrics.total_rl_score",
        "local_schemas": "activeContext, scratchpad, kanban, mistakes, systemPatterns, roadmap, memory use metrics.rl_source_ref = 'progress.json'",
        "schema_updates": "Updated all 8 *.schema.json definitions to enforce this architecture",
        "memory_bank_sync": "Updated all 8 *.json files to remove duplicate total_rl_score fields"
      },
      "research_sources": [
        "Remutable: Server-side single source of truth with client diffs",
        "PyTorch distributed state: One rank collects, all sync",
        "AFRAME: Centralized state with handlers"
      ],
      "benefits": [
        "Zero synchronization errors",
        "Clear authority hierarchy",
        "Simplified updates (one location)",
        "Consistent display across all schemas"
      ],
      "validation": "grep -l total_rl_score *.json should return ONLY progress.json",
      "confidence": 0.95,
      "reuse_count": 0,
      "success_rate": 1.0,
      "rl_reward": 40,
      "created_at": "2025-10-15T12:05:00+05:30",
      "constitutional_articles": ["Art 12 (RL)", "Art 14 (Schema)", "Art 16 (Compliance)"]
    },
    {
      "pattern": "Parallel Browser Tab Scraping with asyncio.Semaphore",
      "implementation": {
        "approach": "5 concurrent browser tabs controlled by asyncio.Semaphore(5)",
        "file": "src/scraper/unified/naukri/browser_scraper.py",
        "functions": ["_enrich_parallel_pages", "_scrape_job_page"],
        "css_selectors": {
          "description": ".styles_JDC__dang-inner-html__h0K4t, [class*='job-description'], .dang-inner-html",
          "skills_container": ".styles_jhc__key-skill__DKjCg, [class*='key-skill'], .key-skill",
          "skills_items": "a, span.chip, .chip-text"
        },
        "performance": "20 jobs in 30.1s with 100% success rate"
      },
      "discovery": "2025-10-14T12:54:50+05:30",
      "rl_reward": 50,
      "success_rate": 1.0,
      "status": "production_ready"
    },
    {
      "pattern": "Naukri API Authentication - Browser Session Required",
      "discovery": "2025-10-14T12:30:00+05:30",
      "authentication_method": {
        "type": "browser_session_cookies",
        "description": "Naukri API v3 requires authenticated browser session cookies, not just headers",
        "headers_insufficient": ["appid", "systemid", "clientid", "gid"],
        "error_without_auth": "406 Not Acceptable",
        "solution": "Use Playwright browser automation to establish session, extract cookies, then use API"
      },
      "implementation": {
        "approach": "Playwright scraper with browser authentication",
        "file": "src/scraper/unified/naukri_unified.py",
        "function": "scrape_naukri_jobs_unified",
        "flow": "Open browser → Establish session → API calls with cookies"
      },
      "rl_learning": "Headers alone = -20 RL failure. Browser auth = working solution.",
      "status": "documented"
    },
    {
      "pattern": "Two-phase Playwright scraping with concurrent execution",
      "phase_1_url_extraction": {
        "browser": "Playwright (headless=False)",
        "concurrency": "5 tabs max",
        "flow": "Open search URL → Scrape job URLs → Deduplicate against jobs table (platform='naukri' + url) → Store to job_url table",
        "implementation": "src/scraper/unified/naukri/url_scraper.py",
        "database": "INSERT into job_url after EXISTS check on jobs table"
      },
      "phase_2_detail_scraping": {
        "browser": "Playwright (headless=False)",
        "concurrency": "5 tabs/batch in single window",
        "flow": "Load URLs from job_url → Open in batches → Extract job details → Parse skills from description → Store to jobs table",
        "implementation": "src/scraper/unified/naukri/detail_scraper.py",
        "skill_extraction": "AdvancedSkillExtractor (557 skills reference)",
        "database": "INSERT into jobs with full details + skills"
      },
      "anti_detection": "headless=False shows visible browser to bypass Naukri bot detection",
      "rl_reward": 20,
      "status": "production_ready"
    },
    {
      "approach": "JobSpy (LinkedIn+Indeed) + Playwright (Naukri) with centralized AdvancedSkillExtractor",
      "implementation": "multi_platform_service.py (89 lines)",
      "platforms": {
        "linkedin": "JobSpy - free, no browser",
        "indeed": "JobSpy - free, no browser",
        "naukri": "Playwright - headless=False for anti-detection"
      },
      "skills": "Single AdvancedSkillExtractor instance (557 skills reference)",
      "status": "production_ready"
    },
    {
      "unified_folder": "Old Playwright scrapers for Indeed - ignore",
      "indeed_unified": "Deleted 115 lines duplicate",
      "service_py": "Old service, replaced by multi_platform_service.py"
    }
  ],
  "emd_compliance": {
    "multi_platform_service": "89 lines (within limit)",
    "benefit": "Single source of truth for skill extraction"
  },
  "cost_savings": {
    "jobspy": "$250+ saved vs BrightData datasets",
    "proxy": "Optional for LinkedIn only, not needed for Indeed/Naukri"
  },
  "mcp_integration": {
    "filesystem": "Schema management",
    "context7": "Documentation lookup",
    "memory": "Pattern storage",
    "git": "Version control",
    "time": "Timestamps",
    "math": "Metrics calculation"
  }
}
