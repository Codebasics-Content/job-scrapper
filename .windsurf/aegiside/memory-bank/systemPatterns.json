{
  "schema_version": "1.0.0",
  "last_updated": "2025-10-14T12:32:12+05:30",
  "metrics": {
    "total_rl_score": -170,
    "tasks_completed": 8,
    "tasks_failed": 1,
    "commits": 8,
    "rl_reward": -70,
    "success_rate": 0.88,
    "gae_advantage": -0.3,
    "value_branch": "learning",
    "timestamp": "2025-10-14T12:32:12+05:30"
  },
  "architecture_patterns": [
    {
      "pattern": "Naukri API Authentication - Browser Session Required",
      "discovery": "2025-10-14T12:30:00+05:30",
      "authentication_method": {
        "type": "browser_session_cookies",
        "description": "Naukri API v3 requires authenticated browser session cookies, not just headers",
        "headers_insufficient": ["appid", "systemid", "clientid", "gid"],
        "error_without_auth": "406 Not Acceptable",
        "solution": "Use Playwright browser automation to establish session, extract cookies, then use API"
      },
      "implementation": {
        "approach": "Playwright scraper with browser authentication",
        "file": "src/scraper/unified/naukri_unified.py",
        "function": "scrape_naukri_jobs_unified",
        "flow": "Open browser → Establish session → API calls with cookies"
      },
      "rl_learning": "Headers alone = -20 RL failure. Browser auth = working solution.",
      "status": "documented"
    },
    {
      "pattern": "Two-phase Playwright scraping with concurrent execution",
      "phase_1_url_extraction": {
        "browser": "Playwright (headless=False)",
        "concurrency": "5 tabs max",
        "flow": "Open search URL → Scrape job URLs → Deduplicate against jobs table (platform='naukri' + url) → Store to job_url table",
        "implementation": "src/scraper/unified/naukri/url_scraper.py",
        "database": "INSERT into job_url after EXISTS check on jobs table"
      },
      "phase_2_detail_scraping": {
        "browser": "Playwright (headless=False)",
        "concurrency": "5 tabs/batch in single window",
        "flow": "Load URLs from job_url → Open in batches → Extract job details → Parse skills from description → Store to jobs table",
        "implementation": "src/scraper/unified/naukri/detail_scraper.py",
        "skill_extraction": "AdvancedSkillExtractor (557 skills reference)",
        "database": "INSERT into jobs with full details + skills"
      },
      "anti_detection": "headless=False shows visible browser to bypass Naukri bot detection",
      "rl_reward": 20,
      "status": "production_ready"
    },
    {
      "approach": "JobSpy (LinkedIn+Indeed) + Playwright (Naukri) with centralized AdvancedSkillExtractor",
      "implementation": "multi_platform_service.py (89 lines)",
      "platforms": {
        "linkedin": "JobSpy - free, no browser",
        "indeed": "JobSpy - free, no browser",
        "naukri": "Playwright - headless=False for anti-detection"
      },
      "skills": "Single AdvancedSkillExtractor instance (557 skills reference)",
      "status": "production_ready"
    },
    {
      "unified_folder": "Old Playwright scrapers for Indeed - ignore",
      "indeed_unified": "Deleted 115 lines duplicate",
      "service_py": "Old service, replaced by multi_platform_service.py"
    }
  ],
  "emd_compliance": {
    "multi_platform_service": "89 lines (within limit)",
    "benefit": "Single source of truth for skill extraction"
  },
  "cost_savings": {
    "jobspy": "$250+ saved vs BrightData datasets",
    "proxy": "Optional for LinkedIn only, not needed for Indeed/Naukri"
  },
  "mcp_integration": {
    "filesystem": "Schema management",
    "context7": "Documentation lookup",
    "memory": "Pattern storage",
    "git": "Version control",
    "time": "Timestamps",
    "math": "Metrics calculation"
  }
}
